{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenADMET Multi-Task Training with Graph Transformer\n",
    "This notebook demonstrates how to train a **Graph Transformer** model on the\n",
    "[OpenADMET](https://openadmet.ghost.io/openadmet-expansionrx-blind-challenge/) multi-task dataset using the `gt-pyg` library.\n",
    "\n",
    "- Graph Transformer in Pytorch Geometric[gt_pyg](https://github.com/pgniewko/gt-pyg)\n",
    "- `v1.6.0.0`\n",
    "\n",
    "> **Note:** This is an exemplary training run for demonstration purposes,\n",
    "> not a production-grade model.\n",
    "\n",
    "**Protocol summary:**\n",
    "- 9 ADMET endpoints trained jointly (sparse labels \u2014 not every molecule has every endpoint)\n",
    "- Single loss term: **MAE** (no weighting, no rescaling)\n",
    "- 250 epochs, **cosine annealing** LR schedule (no warmup)\n",
    "- Track best macro-averaged MAE on the validation set\n",
    "- Evaluate on the held-out test set (public leaderboard + private split)\n",
    "\n",
    "> **Loss caveat:** The plain MAE loss treats all endpoints and all datapoints\n",
    "> uniformly. Endpoints with more observed labels or larger variance therefore\n",
    "> have disproportionate influence on the gradient. A production model would\n",
    "> use per-task normalisation (e.g. RAE) or a multi-objective loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "from sklearn.metrics import r2_score\n",
    "from copy import deepcopy\n",
    "\n",
    "from gt_pyg.data import get_tensor_data, get_atom_feature_dim, get_bond_feature_dim\n",
    "from gt_pyg.nn import GraphTransformerNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "TRAIN_CSV = \"data/train-set/expansion_log_data_train.csv\"\n",
    "TEST_CSV = \"data/test-set/expansion_data_test_full_lb_flag.csv\"\n",
    "\n",
    "# Model\n",
    "HIDDEN_DIM = 128\n",
    "NUM_GT_LAYERS = 4\n",
    "NUM_HEADS = 8\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# Training\n",
    "EPOCHS = 250\n",
    "BATCH_SIZE = 256\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "SEED = 42\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Split Data\n",
    "\n",
    "The training set contains log-transformed ADMET endpoint values. Many values are\n",
    "missing \u2014 this is a **sparse multi-task** problem. We perform a random 80/20\n",
    "train/validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_train_df = pd.read_csv(TRAIN_CSV)\n",
    "\n",
    "ID_COLS = {\"SMILES\", \"Molecule Name\"}\n",
    "ENDPOINTS = [c for c in log_train_df.columns if c not in ID_COLS]\n",
    "NUM_TASKS = len(ENDPOINTS)\n",
    "\n",
    "print(f\"Total molecules: {len(log_train_df)}\")\n",
    "print(f\"Endpoints ({NUM_TASKS}): {ENDPOINTS}\")\n",
    "print(f\"\\nLabel coverage per endpoint:\")\n",
    "for ep in ENDPOINTS:\n",
    "    n = log_train_df[ep].notna().sum()\n",
    "    print(f\"  {ep}: {n} / {len(log_train_df)} ({100*n/len(log_train_df):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and split 80/20\n",
    "df = log_train_df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "n_train = int(0.8 * len(df))\n",
    "tr_df = df.iloc[:n_train].copy()\n",
    "va_df = df.iloc[n_train:].copy()\n",
    "\n",
    "print(f\"Train: {len(tr_df)}, Validation: {len(va_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build PyG Datasets and DataLoaders\n",
    "\n",
    "`get_tensor_data` converts SMILES + labels into PyG `Data` objects with\n",
    "atom/bond features, GNM positional encodings, and multi-task label tensors.\n",
    "Missing labels are encoded as `NaN` with an accompanying binary mask (`y_mask`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(df, endpoints):\n",
    "    \"\"\"Convert a DataFrame into a list of PyG Data objects.\"\"\"\n",
    "    smiles = df[\"SMILES\"].tolist()\n",
    "    labels = df[endpoints].values.tolist()  # list of lists, NaN for missing\n",
    "    return get_tensor_data(smiles, labels)\n",
    "\n",
    "print(\"Building training set...\")\n",
    "tr_dataset = build_dataset(tr_df, ENDPOINTS)\n",
    "print(\"Building validation set...\")\n",
    "va_dataset = build_dataset(va_df, ENDPOINTS)\n",
    "\n",
    "tr_loader = DataLoader(tr_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "va_loader = DataLoader(va_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"\\nTrain batches: {len(tr_loader)}, Val batches: {len(va_loader)}\")\n",
    "print(f\"Node features: {tr_dataset[0].x.shape[1]}, Edge features: {tr_dataset[0].edge_attr.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Model, Optimizer, and Scheduler\n",
    "\n",
    "We use `GraphTransformerNet` with a variational readout head. During evaluation\n",
    "the model returns the deterministic mean (`mu`). The LR follows a **cosine\n",
    "annealing** schedule from `LR` down to 0 over 250 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_dim = get_atom_feature_dim()\n",
    "edge_dim = get_bond_feature_dim()\n",
    "\n",
    "model = GraphTransformerNet(\n",
    "    node_dim_in=node_dim,\n",
    "    edge_dim_in=edge_dim,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_gt_layers=NUM_GT_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT,\n",
    "    num_tasks=NUM_TASKS,\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"Parameters: {model.num_parameters():,}\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training and Evaluation Functions\n",
    "\n",
    "The loss is **masked MAE**: for each sample, only endpoints with observed labels\n",
    "contribute to the loss. This handles the sparse multi-task structure naturally.\n",
    "\n",
    "Evaluation computes the **5 official challenge metrics** per endpoint:\n",
    "MAE, RAE, R\u00b2, Spearman $\\rho$, Kendall $\\tau$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mae_loss(pred, target, mask):\n",
    "    \"\"\"MAE loss computed only over observed (non-NaN) labels.\"\"\"\n",
    "    diff = torch.abs(pred - target) * mask\n",
    "    count = mask.sum()\n",
    "    if count == 0:\n",
    "        return torch.tensor(0.0, device=pred.device)\n",
    "    return diff.sum() / count\n",
    "\n",
    "\n",
    "def train_epoch(model, loader, optimizer, device):\n",
    "    \"\"\"Run one training epoch. Returns average loss.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred, _ = model(\n",
    "            x=batch.x,\n",
    "            edge_index=batch.edge_index,\n",
    "            edge_attr=batch.edge_attr,\n",
    "            batch=batch.batch,\n",
    "        )\n",
    "\n",
    "        target = batch.y\n",
    "        mask = batch.y_mask\n",
    "        # Replace NaN in target with 0 to avoid NaN in loss\n",
    "        target = torch.nan_to_num(target, nan=0.0)\n",
    "\n",
    "        loss = masked_mae_loss(pred, target, mask)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    return total_loss / max(n_batches, 1)\n",
    "\n",
    "\n",
    "def _official_metrics(y_true_1d, y_pred_1d):\n",
    "    \"\"\"Compute the 5 official challenge metrics for a single endpoint.\"\"\"\n",
    "    y = np.asarray(y_true_1d).ravel()\n",
    "    p = np.asarray(y_pred_1d).ravel()\n",
    "    m = np.isfinite(y) & np.isfinite(p)\n",
    "    y = y[m]\n",
    "    p = p[m]\n",
    "    if y.size == 0:\n",
    "        return {\n",
    "            \"MAE\": np.nan,\n",
    "            \"RAE\": np.nan,\n",
    "            \"R2\": np.nan,\n",
    "            \"Spearman R\": np.nan,\n",
    "            \"Kendall's Tau\": np.nan,\n",
    "        }\n",
    "    mae = float(np.mean(np.abs(y - p)))\n",
    "    denom = np.mean(np.abs(y - np.mean(y))) if y.size > 0 else np.nan\n",
    "    rae = float(mae / denom) if denom and np.isfinite(denom) and denom > 0 else np.nan\n",
    "    r2 = float(r2_score(y, p)) if (np.nanstd(y) > 0) else np.nan\n",
    "    if np.nanstd(p) < 1e-4:\n",
    "        spr = np.nan\n",
    "        ktau = np.nan\n",
    "    else:\n",
    "        spr = float(spearmanr(y, p)[0])\n",
    "        ktau = float(kendalltau(y, p)[0])\n",
    "    return {\n",
    "        \"MAE\": mae,\n",
    "        \"RAE\": rae,\n",
    "        \"R2\": r2,\n",
    "        \"Spearman R\": spr,\n",
    "        \"Kendall's Tau\": ktau,\n",
    "    }\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device, endpoints):\n",
    "    \"\"\"Evaluate model. Returns (metrics_dict, preds, targets, masks).\n",
    "\n",
    "    metrics_dict[endpoint] has 5 official metrics + N.\n",
    "    metrics_dict[\"Average\"] has macro-averaged values (nanmean).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_masks = []\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        pred, _ = model(\n",
    "            x=batch.x,\n",
    "            edge_index=batch.edge_index,\n",
    "            edge_attr=batch.edge_attr,\n",
    "            batch=batch.batch,\n",
    "        )\n",
    "        all_preds.append(pred.cpu())\n",
    "        all_targets.append(batch.y.cpu())\n",
    "        all_masks.append(batch.y_mask.cpu())\n",
    "\n",
    "    preds = torch.cat(all_preds, dim=0).numpy()\n",
    "    targets = torch.cat(all_targets, dim=0).numpy()\n",
    "    masks = torch.cat(all_masks, dim=0).numpy()\n",
    "\n",
    "    metrics = {}\n",
    "    for i, ep in enumerate(endpoints):\n",
    "        valid = masks[:, i] > 0.5\n",
    "        if valid.sum() < 2:\n",
    "            continue\n",
    "        y = targets[valid, i]\n",
    "        p = preds[valid, i]\n",
    "        official = _official_metrics(y, p)\n",
    "        official[\"N\"] = int(valid.sum())\n",
    "        metrics[ep] = official\n",
    "\n",
    "    # Macro averages (nanmean over endpoints)\n",
    "    keys = [\"MAE\", \"RAE\", \"R2\", \"Spearman R\", \"Kendall's Tau\"]\n",
    "    avg = {}\n",
    "    for k in keys:\n",
    "        vals = [metrics[ep][k] for ep in metrics if ep != \"Average\"]\n",
    "        avg[k] = float(np.nanmean(vals)) if vals else np.nan\n",
    "    metrics[\"Average\"] = avg\n",
    "\n",
    "    return metrics, preds, targets, masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Loop\n",
    "\n",
    "We train for 250 epochs, tracking the **best macro-averaged MAE** on the\n",
    "validation set. The best model weights are stored in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_mae = float(\"inf\")\n",
    "best_model_state = None\n",
    "best_epoch = -1\n",
    "\n",
    "history = {\n",
    "    \"epoch\": [], \"train_loss\": [], \"lr\": [],\n",
    "    \"val_mae\": [], \"val_rae\": [], \"val_r2\": [],\n",
    "    \"val_spearman\": [], \"val_kendall\": [],\n",
    "    # Train metrics recorded at logged epochs only\n",
    "    \"train_mae\": [], \"train_rae\": [], \"train_r2\": [],\n",
    "    \"train_spearman\": [], \"train_kendall\": [],\n",
    "    \"logged_epoch\": [],\n",
    "}\n",
    "\n",
    "KT_KEY = \"Kendall's Tau\"\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss = train_epoch(model, tr_loader, optimizer, DEVICE)\n",
    "    scheduler.step()\n",
    "\n",
    "    val_metrics_ep, _, _, _ = evaluate(model, va_loader, DEVICE, ENDPOINTS)\n",
    "\n",
    "    val_avg = val_metrics_ep[\"Average\"]\n",
    "    val_mae = val_avg[\"MAE\"]\n",
    "    lr_now = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "    history[\"epoch\"].append(epoch)\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"lr\"].append(lr_now)\n",
    "    history[\"val_mae\"].append(val_mae)\n",
    "    history[\"val_rae\"].append(val_avg[\"RAE\"])\n",
    "    history[\"val_r2\"].append(val_avg[\"R2\"])\n",
    "    history[\"val_spearman\"].append(val_avg[\"Spearman R\"])\n",
    "    history[\"val_kendall\"].append(val_avg[KT_KEY])\n",
    "\n",
    "    if val_mae < best_val_mae:\n",
    "        best_val_mae = val_mae\n",
    "        best_epoch = epoch\n",
    "        best_model_state = deepcopy(model.state_dict())\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        tr_metrics_ep, _, _, _ = evaluate(model, tr_loader, DEVICE, ENDPOINTS)\n",
    "        tr_avg = tr_metrics_ep[\"Average\"]\n",
    "\n",
    "        history[\"logged_epoch\"].append(epoch)\n",
    "        history[\"train_mae\"].append(tr_avg[\"MAE\"])\n",
    "        history[\"train_rae\"].append(tr_avg[\"RAE\"])\n",
    "        history[\"train_r2\"].append(tr_avg[\"R2\"])\n",
    "        history[\"train_spearman\"].append(tr_avg[\"Spearman R\"])\n",
    "        history[\"train_kendall\"].append(tr_avg[KT_KEY])\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:3d}/{EPOCHS} | \"\n",
    "            f\"Loss: {train_loss:.4f} | \"\n",
    "            f\"Train MAE={tr_avg['MAE']:.3f} RAE={tr_avg['RAE']:.3f} \"\n",
    "            f\"R2={tr_avg['R2']:.3f} rho={tr_avg['Spearman R']:.3f} \"\n",
    "            f\"tau={tr_avg[KT_KEY]:.3f} | \"\n",
    "            f\"Val MAE={val_avg['MAE']:.3f} RAE={val_avg['RAE']:.3f} \"\n",
    "            f\"R2={val_avg['R2']:.3f} rho={val_avg['Spearman R']:.3f} \"\n",
    "            f\"tau={val_avg[KT_KEY]:.3f} | \"\n",
    "            f\"LR: {lr_now:.2e}\"\n",
    "        )\n",
    "\n",
    "print(f\"\\nBest val MAE: {best_val_mae:.4f} at epoch {best_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Restore Best Model and Print Final Stats\n",
    "\n",
    "Load the best checkpoint (by validation MAE) and report per-endpoint metrics\n",
    "on both train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "tr_metrics, _, _, _ = evaluate(model, tr_loader, DEVICE, ENDPOINTS)\n",
    "va_metrics, _, _, _ = evaluate(model, va_loader, DEVICE, ENDPOINTS)\n",
    "\n",
    "header = (\n",
    "    f\"{'Endpoint':<22} \"\n",
    "    f\"{'Train MAE':>10} {'Val MAE':>10} \"\n",
    "    f\"{'Train RAE':>10} {'Val RAE':>10} \"\n",
    "    f\"{'Train R2':>10} {'Val R2':>10} \"\n",
    "    f\"{'Train rho':>10} {'Val rho':>10} \"\n",
    "    f\"{'Train tau':>10} {'Val tau':>10} \"\n",
    "    f\"{'N_val':>7}\"\n",
    ")\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "KT_KEY = \"Kendall's Tau\"\n",
    "\n",
    "for ep in ENDPOINTS:\n",
    "    if ep in tr_metrics and ep in va_metrics:\n",
    "        tm = tr_metrics[ep]\n",
    "        vm = va_metrics[ep]\n",
    "        print(\n",
    "            f\"{ep:<22} \"\n",
    "            f\"{tm['MAE']:>10.4f} {vm['MAE']:>10.4f} \"\n",
    "            f\"{tm['RAE']:>10.4f} {vm['RAE']:>10.4f} \"\n",
    "            f\"{tm['R2']:>10.4f} {vm['R2']:>10.4f} \"\n",
    "            f\"{tm['Spearman R']:>10.4f} {vm['Spearman R']:>10.4f} \"\n",
    "            f\"{tm[KT_KEY]:>10.4f} {vm[KT_KEY]:>10.4f} \"\n",
    "            f\"{vm['N']:>7d}\"\n",
    "        )\n",
    "\n",
    "print(\"-\" * len(header))\n",
    "ta = tr_metrics[\"Average\"]\n",
    "va = va_metrics[\"Average\"]\n",
    "print(\n",
    "    f\"{'Average':<22} \"\n",
    "    f\"{ta['MAE']:>10.4f} {va['MAE']:>10.4f} \"\n",
    "    f\"{ta['RAE']:>10.4f} {va['RAE']:>10.4f} \"\n",
    "    f\"{ta['R2']:>10.4f} {va['R2']:>10.4f} \"\n",
    "    f\"{ta['Spearman R']:>10.4f} {va['Spearman R']:>10.4f} \"\n",
    "    f\"{ta[KT_KEY]:>10.4f} {va[KT_KEY]:>10.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# ---- (0,0) Loss curves (LOG SCALE) ----\n",
    "ax = axes[0, 0]\n",
    "ax.semilogy(history[\"epoch\"], history[\"train_loss\"], label=\"Train Loss\", alpha=0.8)\n",
    "# Compute val loss from val_mae as proxy (we don't track val loss separately)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss (log scale)\")\n",
    "ax.set_title(\"Training Loss\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# ---- (0,1) RAE (capped at 1.0) ----\n",
    "ax = axes[0, 1]\n",
    "ax.plot(history[\"epoch\"], history[\"val_rae\"], \"b-\", alpha=0.8, label=\"Val RAE\")\n",
    "if history[\"logged_epoch\"]:\n",
    "    ax.plot(history[\"logged_epoch\"], history[\"train_rae\"], \"b--\", alpha=0.5, label=\"Train RAE\")\n",
    "best_rae = min(history[\"val_rae\"])\n",
    "ax.axhline(best_rae, color=\"r\", linestyle=\"--\", alpha=0.5, label=f\"Best Val: {best_rae:.3f}\")\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"RAE\")\n",
    "ax.set_title(\"RAE (capped at 1.0)\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# ---- (1,0) R\u00b2 (capped at 1.0) ----\n",
    "ax = axes[1, 0]\n",
    "ax.plot(history[\"epoch\"], history[\"val_r2\"], \"g-\", alpha=0.8, label=\"Val R\u00b2\")\n",
    "if history[\"logged_epoch\"]:\n",
    "    ax.plot(history[\"logged_epoch\"], history[\"train_r2\"], \"g--\", alpha=0.5, label=\"Train R\u00b2\")\n",
    "best_r2 = max(history[\"val_r2\"])\n",
    "ax.axhline(best_r2, color=\"r\", linestyle=\"--\", alpha=0.5, label=f\"Best Val: {best_r2:.3f}\")\n",
    "ax.set_ylim(-0.1, 1.0)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"R\u00b2\")\n",
    "ax.set_title(\"R\u00b2 (capped at 1.0)\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# ---- (1,1) Kendall \u03c4 ----\n",
    "ax = axes[1, 1]\n",
    "ax.plot(history[\"epoch\"], history[\"val_kendall\"], \"m-\", alpha=0.8, label=\"Val \u03c4\")\n",
    "if history[\"logged_epoch\"]:\n",
    "    ax.plot(history[\"logged_epoch\"], history[\"train_kendall\"], \"m--\", alpha=0.5, label=\"Train \u03c4\")\n",
    "best_tau = max(history[\"val_kendall\"])\n",
    "ax.axhline(best_tau, color=\"r\", linestyle=\"--\", alpha=0.5, label=f\"Best Val: {best_tau:.3f}\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Kendall \u03c4\")\n",
    "ax.set_title(\"Kendall \u03c4\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Bar Plots \u2014 Train vs Validation MAE and R\u00b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build metrics dataframe for bar plots\n",
    "metric_records = []\n",
    "for ep in ENDPOINTS:\n",
    "    tm = tr_metrics.get(ep, {})\n",
    "    vm = va_metrics.get(ep, {})\n",
    "    metric_records.append({\"Assay\": ep, \"Split\": \"Train\", \"R2\": tm.get(\"R2\", np.nan), \"MAE\": tm.get(\"MAE\", np.nan)})\n",
    "    metric_records.append({\"Assay\": ep, \"Split\": \"Val\", \"R2\": vm.get(\"R2\", np.nan), \"MAE\": vm.get(\"MAE\", np.nan)})\n",
    "\n",
    "metrics_df = pd.DataFrame(metric_records)\n",
    "\n",
    "# Order assays by validation MAE ascending\n",
    "order_by_val = (\n",
    "    metrics_df[metrics_df[\"Split\"] == \"Val\"]\n",
    "    .sort_values(\"MAE\")[\"Assay\"]\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# ---- MAE bar plot ----\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "sns.barplot(x=\"Assay\", y=\"MAE\", hue=\"Split\", data=metrics_df, ax=ax, order=order_by_val)\n",
    "ax.set_ylabel(\"MAE (log-space)\")\n",
    "ax.set_title(\"Training vs Validation MAE per Endpoint\")\n",
    "ax.set_xticklabels([t.get_text().replace(\"_\", \"\\n\") for t in ax.get_xticklabels()])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- R\u00b2 bar plot ----\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "sns.barplot(x=\"Assay\", y=\"R2\", hue=\"Split\", data=metrics_df, ax=ax, order=order_by_val)\n",
    "ax.set_ylim(-1.1, 1.0)\n",
    "ax.set_ylabel(\"$R^2$\")\n",
    "ax.set_title(\"Training vs Validation R\u00b2 per Endpoint\")\n",
    "ax.set_xticklabels([t.get_text().replace(\"_\", \"\\n\") for t in ax.get_xticklabels()])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test Set Prediction & Submission\n",
    "\n",
    "Generate predictions on the held-out test set and save as `submission.csv`.\n",
    "The model predicts in **log-space** (matching the training labels), and we\n",
    "inverse-transform to **original assay units** for the submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load test set ---\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "print(f\"Columns: {test_df.columns.tolist()}\")\n",
    "\n",
    "smiles_test = test_df[\"SMILES\"].tolist()\n",
    "\n",
    "# --- Build PyG dataset (inference \u2014 no labels) ---\n",
    "print(\"\\nFeaturising test SMILES...\")\n",
    "test_dataset = get_tensor_data(smiles_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# --- Run inference ---\n",
    "model.eval()\n",
    "test_preds_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = batch.to(DEVICE)\n",
    "        pred, _ = model(\n",
    "            x=batch.x,\n",
    "            edge_index=batch.edge_index,\n",
    "            edge_attr=batch.edge_attr,\n",
    "            batch=batch.batch,\n",
    "        )\n",
    "        test_preds_list.append(pred.cpu().numpy())\n",
    "\n",
    "test_preds_log = np.concatenate(test_preds_list, axis=0)  # [N, 9] in log-space\n",
    "print(f\"Predictions shape: {test_preds_log.shape}\")\n",
    "\n",
    "# --- Inverse-transform from log-space to original assay units ---\n",
    "# Mapping: log-space endpoint name -> (submission column name, transform)\n",
    "INVERSE_MAP = {\n",
    "    \"LogD\":             (\"LogD\",                           lambda x: x),         # already log-scale\n",
    "    \"LogS\":             (\"KSOL\",                           lambda x: 10**x),\n",
    "    \"Log_HLM_CLint\":    (\"HLM CLint\",                     lambda x: 10**x),\n",
    "    \"Log_MLM_CLint\":    (\"MLM CLint\",                     lambda x: 10**x),\n",
    "    \"Log_Caco_Papp_AB\": (\"Caco-2 Permeability Papp A>B\",  lambda x: 10**x),\n",
    "    \"Log_Caco_ER\":      (\"Caco-2 Permeability Efflux\",    lambda x: 10**x),\n",
    "    \"Log_Mouse_PPB\":    (\"MPPB\",                           lambda x: 10**x),\n",
    "    \"Log_Mouse_BPB\":    (\"MBPB\",                           lambda x: 10**x),\n",
    "    \"Log_Mouse_MPB\":    (\"MGMB\",                           lambda x: 10**x),\n",
    "}\n",
    "\n",
    "# Build submission DataFrame\n",
    "sub_df = test_df[[\"Molecule Name\", \"SMILES\"]].copy()\n",
    "\n",
    "for i, ep_log in enumerate(ENDPOINTS):\n",
    "    col_name, transform = INVERSE_MAP[ep_log]\n",
    "    sub_df[col_name] = transform(test_preds_log[:, i])\n",
    "\n",
    "sub_df.to_csv(\"submission.csv\", index=False)\n",
    "print(f\"\\nSaved submission.csv with {len(sub_df)} rows\")\n",
    "print(f\"Columns: {sub_df.columns.tolist()}\\n\")\n",
    "\n",
    "# --- Show head and summary ---\n",
    "print(\"Head:\")\n",
    "print(sub_df.head().to_string())\n",
    "\n",
    "print(\"\\nPrediction range summary (original scale):\")\n",
    "for col in sub_df.columns:\n",
    "    if col not in (\"Molecule Name\", \"SMILES\"):\n",
    "        vals = sub_df[col]\n",
    "        print(f\"  {col:<35s}  min={vals.min():.4f}  max={vals.max():.4f}  mean={vals.mean():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gt-pyg)",
   "language": "python",
   "name": "gt-pyg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

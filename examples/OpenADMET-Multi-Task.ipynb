{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenADMET Multi-Task Training with Graph Transformer\n",
    "This notebook demonstrates how to train a **Graph Transformer** model on the\n",
    "[OpenADMET](https://openadmet.ghost.io/openadmet-expansionrx-blind-challenge/) multi-task dataset using the `gt-pyg` library.\n",
    "\n",
    "- Graph Transformer in Pytorch Geometric[gt_pyg](https://github.com/pgniewko/gt-pyg)\n",
    "- `v1.6.0.0`\n",
    "\n",
    "> **Note:** This is an exemplary training run for demonstration purposes,\n",
    "> not a production-grade model.\n",
    "\n",
    "**Protocol summary:**\n",
    "- 9 ADMET endpoints trained jointly (sparse labels — not every molecule has every endpoint)\n",
    "- Single loss term: **MAE** (no weighting, no rescaling)\n",
    "- 250 epochs, **cosine annealing** LR schedule (no warmup)\n",
    "- Track best macro-averaged MAE on the validation set\n",
    "- Evaluate on the held-out test set (public leaderboard + private split)\n",
    "\n",
    "> **Loss caveat:** The plain MAE loss treats all endpoints and all datapoints\n",
    "> uniformly. Endpoints with more observed labels or larger variance therefore\n",
    "> have disproportionate influence on the gradient. A production model would\n",
    "> use per-task normalisation (e.g. RAE) or a multi-objective loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "from sklearn.metrics import r2_score\n",
    "from copy import deepcopy\n",
    "\n",
    "from gt_pyg.data import get_tensor_data, get_atom_feature_dim, get_bond_feature_dim\n",
    "from gt_pyg.nn import GraphTransformerNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "TRAIN_CSV = \"data/train-set/expansion_log_data_train.csv\"\n",
    "TEST_CSV = \"data/test-set/expansion_data_test_full_lb_flag.csv\"\n",
    "\n",
    "# Model\n",
    "HIDDEN_DIM = 128\n",
    "NUM_GT_LAYERS = 4\n",
    "NUM_HEADS = 8\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# Training\n",
    "EPOCHS = 250\n",
    "BATCH_SIZE = 256\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "SEED = 42\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Split Data\n",
    "\n",
    "The training set contains log-transformed ADMET endpoint values. Many values are\n",
    "missing — this is a **sparse multi-task** problem. We perform a random 80/20\n",
    "train/validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total molecules: 5326\n",
      "Endpoints (9): ['LogD', 'LogS', 'Log_HLM_CLint', 'Log_MLM_CLint', 'Log_Caco_Papp_AB', 'Log_Caco_ER', 'Log_Mouse_PPB', 'Log_Mouse_BPB', 'Log_Mouse_MPB']\n",
      "\n",
      "Label coverage per endpoint:\n",
      "  LogD: 5039 / 5326 (94.6%)\n",
      "  LogS: 5128 / 5326 (96.3%)\n",
      "  Log_HLM_CLint: 3759 / 5326 (70.6%)\n",
      "  Log_MLM_CLint: 4522 / 5326 (84.9%)\n",
      "  Log_Caco_Papp_AB: 2157 / 5326 (40.5%)\n",
      "  Log_Caco_ER: 2161 / 5326 (40.6%)\n",
      "  Log_Mouse_PPB: 1302 / 5326 (24.4%)\n",
      "  Log_Mouse_BPB: 975 / 5326 (18.3%)\n",
      "  Log_Mouse_MPB: 222 / 5326 (4.2%)\n"
     ]
    }
   ],
   "source": [
    "log_train_df = pd.read_csv(TRAIN_CSV)\n",
    "\n",
    "ID_COLS = {\"SMILES\", \"Molecule Name\"}\n",
    "ENDPOINTS = [c for c in log_train_df.columns if c not in ID_COLS]\n",
    "NUM_TASKS = len(ENDPOINTS)\n",
    "\n",
    "print(f\"Total molecules: {len(log_train_df)}\")\n",
    "print(f\"Endpoints ({NUM_TASKS}): {ENDPOINTS}\")\n",
    "print(f\"\\nLabel coverage per endpoint:\")\n",
    "for ep in ENDPOINTS:\n",
    "    n = log_train_df[ep].notna().sum()\n",
    "    print(f\"  {ep}: {n} / {len(log_train_df)} ({100*n/len(log_train_df):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 4260, Validation: 1066\n"
     ]
    }
   ],
   "source": [
    "# Shuffle and split 80/20\n",
    "df = log_train_df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "n_train = int(0.8 * len(df))\n",
    "tr_df = df.iloc[:n_train].copy()\n",
    "va_df = df.iloc[n_train:].copy()\n",
    "\n",
    "print(f\"Train: {len(tr_df)}, Validation: {len(va_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build PyG Datasets and DataLoaders\n",
    "\n",
    "`get_tensor_data` converts SMILES + labels into PyG `Data` objects with\n",
    "atom/bond features, GNM positional encodings, and multi-task label tensors.\n",
    "Missing labels are encoded as `NaN` with an accompanying binary mask (`y_mask`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building training set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  14%|█████████████▋                                                                                     | 591/4260 [00:00<00:05, 624.38it/s]/Users/pawelgniewek/projects/gt-pyg/gt_pyg/data/utils.py:236: RuntimeWarning: divide by zero encountered in divide\n",
      "  inv_eigenvalues = np.where(np.abs(eigenvalues) > 1e-10, 1.0 / eigenvalues, 0.0)\n",
      "Processing data: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 4260/4260 [00:06<00:00, 665.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 1066/1066 [00:01<00:00, 649.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train batches: 17, Val batches: 5\n",
      "Node features: 139, Edge features: 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(df, endpoints):\n",
    "    \"\"\"Convert a DataFrame into a list of PyG Data objects.\"\"\"\n",
    "    smiles = df[\"SMILES\"].tolist()\n",
    "    labels = df[endpoints].values.tolist()  # list of lists, NaN for missing\n",
    "    return get_tensor_data(smiles, labels)\n",
    "\n",
    "print(\"Building training set...\")\n",
    "tr_dataset = build_dataset(tr_df, ENDPOINTS)\n",
    "print(\"Building validation set...\")\n",
    "va_dataset = build_dataset(va_df, ENDPOINTS)\n",
    "\n",
    "tr_loader = DataLoader(tr_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "va_loader = DataLoader(va_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"\\nTrain batches: {len(tr_loader)}, Val batches: {len(va_loader)}\")\n",
    "print(f\"Node features: {tr_dataset[0].x.shape[1]}, Edge features: {tr_dataset[0].edge_attr.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Model, Optimizer, and Scheduler\n",
    "\n",
    "We use `GraphTransformerNet` with a variational readout head. During evaluation\n",
    "the model returns the deterministic mean (`mu`). The LR follows a **cosine\n",
    "annealing** schedule from `LR` down to 0 over 250 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 2,564,914\n"
     ]
    }
   ],
   "source": [
    "node_dim = get_atom_feature_dim()\n",
    "edge_dim = get_bond_feature_dim()\n",
    "\n",
    "model = GraphTransformerNet(\n",
    "    node_dim_in=node_dim,\n",
    "    edge_dim_in=edge_dim,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_gt_layers=NUM_GT_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT,\n",
    "    num_tasks=NUM_TASKS,\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"Parameters: {model.num_parameters():,}\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training and Evaluation Functions\n",
    "\n",
    "The loss is **masked MAE**: for each sample, only endpoints with observed labels\n",
    "contribute to the loss. This handles the sparse multi-task structure naturally.\n",
    "\n",
    "Evaluation computes the **5 official challenge metrics** per endpoint:\n",
    "MAE, RAE, R², Spearman $\\rho$, Kendall $\\tau$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mae_loss(pred, target, mask):\n",
    "    \"\"\"MAE loss computed only over observed (non-NaN) labels.\"\"\"\n",
    "    diff = torch.abs(pred - target) * mask\n",
    "    count = mask.sum()\n",
    "    if count == 0:\n",
    "        return torch.tensor(0.0, device=pred.device)\n",
    "    return diff.sum() / count\n",
    "\n",
    "\n",
    "def train_epoch(model, loader, optimizer, device):\n",
    "    \"\"\"Run one training epoch. Returns average loss.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred, _ = model(\n",
    "            x=batch.x,\n",
    "            edge_index=batch.edge_index,\n",
    "            edge_attr=batch.edge_attr,\n",
    "            batch=batch.batch,\n",
    "        )\n",
    "\n",
    "        target = batch.y\n",
    "        mask = batch.y_mask\n",
    "        # Replace NaN in target with 0 to avoid NaN in loss\n",
    "        target = torch.nan_to_num(target, nan=0.0)\n",
    "\n",
    "        loss = masked_mae_loss(pred, target, mask)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    return total_loss / max(n_batches, 1)\n",
    "\n",
    "\n",
    "def _official_metrics(y_true_1d, y_pred_1d):\n",
    "    \"\"\"Compute the 5 official challenge metrics for a single endpoint.\"\"\"\n",
    "    y = np.asarray(y_true_1d).ravel()\n",
    "    p = np.asarray(y_pred_1d).ravel()\n",
    "    m = np.isfinite(y) & np.isfinite(p)\n",
    "    y = y[m]\n",
    "    p = p[m]\n",
    "    if y.size == 0:\n",
    "        return {\n",
    "            \"MAE\": np.nan,\n",
    "            \"RAE\": np.nan,\n",
    "            \"R2\": np.nan,\n",
    "            \"Spearman R\": np.nan,\n",
    "            \"Kendall's Tau\": np.nan,\n",
    "        }\n",
    "    mae = float(np.mean(np.abs(y - p)))\n",
    "    denom = np.mean(np.abs(y - np.mean(y))) if y.size > 0 else np.nan\n",
    "    rae = float(mae / denom) if denom and np.isfinite(denom) and denom > 0 else np.nan\n",
    "    r2 = float(r2_score(y, p)) if (np.nanstd(y) > 0) else np.nan\n",
    "    if np.nanstd(p) < 1e-4:\n",
    "        spr = np.nan\n",
    "        ktau = np.nan\n",
    "    else:\n",
    "        spr = float(spearmanr(y, p)[0])\n",
    "        ktau = float(kendalltau(y, p)[0])\n",
    "    return {\n",
    "        \"MAE\": mae,\n",
    "        \"RAE\": rae,\n",
    "        \"R2\": r2,\n",
    "        \"Spearman R\": spr,\n",
    "        \"Kendall's Tau\": ktau,\n",
    "    }\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device, endpoints):\n",
    "    \"\"\"Evaluate model. Returns (metrics_dict, preds, targets, masks).\n",
    "\n",
    "    metrics_dict[endpoint] has 5 official metrics + N.\n",
    "    metrics_dict[\"Average\"] has macro-averaged values (nanmean).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_masks = []\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        pred, _ = model(\n",
    "            x=batch.x,\n",
    "            edge_index=batch.edge_index,\n",
    "            edge_attr=batch.edge_attr,\n",
    "            batch=batch.batch,\n",
    "        )\n",
    "        all_preds.append(pred.cpu())\n",
    "        all_targets.append(batch.y.cpu())\n",
    "        all_masks.append(batch.y_mask.cpu())\n",
    "\n",
    "    preds = torch.cat(all_preds, dim=0).numpy()\n",
    "    targets = torch.cat(all_targets, dim=0).numpy()\n",
    "    masks = torch.cat(all_masks, dim=0).numpy()\n",
    "\n",
    "    metrics = {}\n",
    "    for i, ep in enumerate(endpoints):\n",
    "        valid = masks[:, i] > 0.5\n",
    "        if valid.sum() < 2:\n",
    "            continue\n",
    "        y = targets[valid, i]\n",
    "        p = preds[valid, i]\n",
    "        official = _official_metrics(y, p)\n",
    "        official[\"N\"] = int(valid.sum())\n",
    "        metrics[ep] = official\n",
    "\n",
    "    # Macro averages (nanmean over endpoints)\n",
    "    keys = [\"MAE\", \"RAE\", \"R2\", \"Spearman R\", \"Kendall's Tau\"]\n",
    "    avg = {}\n",
    "    for k in keys:\n",
    "        vals = [metrics[ep][k] for ep in metrics if ep != \"Average\"]\n",
    "        avg[k] = float(np.nanmean(vals)) if vals else np.nan\n",
    "    metrics[\"Average\"] = avg\n",
    "\n",
    "    return metrics, preds, targets, masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Loop\n",
    "\n",
    "We train for 250 epochs, tracking the **best macro-averaged MAE** on the\n",
    "validation set. The best model weights are stored in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/250 | Loss: 1.0036 | Train MAE=0.489 RAE=1.037 R2=-0.145 rho=0.111 tau=0.074 | Val MAE=0.492 RAE=1.032 R2=-0.131 rho=0.110 tau=0.075 | LR: 1.00e-03\n"
     ]
    }
   ],
   "source": [
    "best_val_mae = float(\"inf\")\n",
    "best_model_state = None\n",
    "best_epoch = -1\n",
    "\n",
    "history = {\n",
    "    \"epoch\": [], \"train_loss\": [], \"lr\": [],\n",
    "    \"val_mae\": [], \"val_rae\": [], \"val_r2\": [],\n",
    "    \"val_spearman\": [], \"val_kendall\": [],\n",
    "    # Train metrics recorded at logged epochs only\n",
    "    \"train_mae\": [], \"train_rae\": [], \"train_r2\": [],\n",
    "    \"train_spearman\": [], \"train_kendall\": [],\n",
    "    \"logged_epoch\": [],\n",
    "}\n",
    "\n",
    "KT_KEY = \"Kendall's Tau\"\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss = train_epoch(model, tr_loader, optimizer, DEVICE)\n",
    "    scheduler.step()\n",
    "\n",
    "    val_metrics_ep, _, _, _ = evaluate(model, va_loader, DEVICE, ENDPOINTS)\n",
    "\n",
    "    val_avg = val_metrics_ep[\"Average\"]\n",
    "    val_mae = val_avg[\"MAE\"]\n",
    "    lr_now = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "    history[\"epoch\"].append(epoch)\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"lr\"].append(lr_now)\n",
    "    history[\"val_mae\"].append(val_mae)\n",
    "    history[\"val_rae\"].append(val_avg[\"RAE\"])\n",
    "    history[\"val_r2\"].append(val_avg[\"R2\"])\n",
    "    history[\"val_spearman\"].append(val_avg[\"Spearman R\"])\n",
    "    history[\"val_kendall\"].append(val_avg[KT_KEY])\n",
    "\n",
    "    if val_mae < best_val_mae:\n",
    "        best_val_mae = val_mae\n",
    "        best_epoch = epoch\n",
    "        best_model_state = deepcopy(model.state_dict())\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        tr_metrics_ep, _, _, _ = evaluate(model, tr_loader, DEVICE, ENDPOINTS)\n",
    "        tr_avg = tr_metrics_ep[\"Average\"]\n",
    "\n",
    "        history[\"logged_epoch\"].append(epoch)\n",
    "        history[\"train_mae\"].append(tr_avg[\"MAE\"])\n",
    "        history[\"train_rae\"].append(tr_avg[\"RAE\"])\n",
    "        history[\"train_r2\"].append(tr_avg[\"R2\"])\n",
    "        history[\"train_spearman\"].append(tr_avg[\"Spearman R\"])\n",
    "        history[\"train_kendall\"].append(tr_avg[KT_KEY])\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:3d}/{EPOCHS} | \"\n",
    "            f\"Loss: {train_loss:.4f} | \"\n",
    "            f\"Train MAE={tr_avg['MAE']:.3f} RAE={tr_avg['RAE']:.3f} \"\n",
    "            f\"R2={tr_avg['R2']:.3f} rho={tr_avg['Spearman R']:.3f} \"\n",
    "            f\"tau={tr_avg[KT_KEY]:.3f} | \"\n",
    "            f\"Val MAE={val_avg['MAE']:.3f} RAE={val_avg['RAE']:.3f} \"\n",
    "            f\"R2={val_avg['R2']:.3f} rho={val_avg['Spearman R']:.3f} \"\n",
    "            f\"tau={val_avg[KT_KEY]:.3f} | \"\n",
    "            f\"LR: {lr_now:.2e}\"\n",
    "        )\n",
    "\n",
    "print(f\"\\nBest val MAE: {best_val_mae:.4f} at epoch {best_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Restore Best Model and Print Final Stats\n",
    "\n",
    "Load the best checkpoint (by validation MAE) and report per-endpoint metrics\n",
    "on both train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "tr_metrics, _, _, _ = evaluate(model, tr_loader, DEVICE, ENDPOINTS)\n",
    "va_metrics, _, _, _ = evaluate(model, va_loader, DEVICE, ENDPOINTS)\n",
    "\n",
    "header = (\n",
    "    f\"{'Endpoint':<22} \"\n",
    "    f\"{'Train MAE':>10} {'Val MAE':>10} \"\n",
    "    f\"{'Train RAE':>10} {'Val RAE':>10} \"\n",
    "    f\"{'Train R2':>10} {'Val R2':>10} \"\n",
    "    f\"{'Train rho':>10} {'Val rho':>10} \"\n",
    "    f\"{'Train tau':>10} {'Val tau':>10} \"\n",
    "    f\"{'N_val':>7}\"\n",
    ")\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "KT_KEY = \"Kendall's Tau\"\n",
    "\n",
    "for ep in ENDPOINTS:\n",
    "    if ep in tr_metrics and ep in va_metrics:\n",
    "        tm = tr_metrics[ep]\n",
    "        vm = va_metrics[ep]\n",
    "        print(\n",
    "            f\"{ep:<22} \"\n",
    "            f\"{tm['MAE']:>10.4f} {vm['MAE']:>10.4f} \"\n",
    "            f\"{tm['RAE']:>10.4f} {vm['RAE']:>10.4f} \"\n",
    "            f\"{tm['R2']:>10.4f} {vm['R2']:>10.4f} \"\n",
    "            f\"{tm['Spearman R']:>10.4f} {vm['Spearman R']:>10.4f} \"\n",
    "            f\"{tm[KT_KEY]:>10.4f} {vm[KT_KEY]:>10.4f} \"\n",
    "            f\"{vm['N']:>7d}\"\n",
    "        )\n",
    "\n",
    "print(\"-\" * len(header))\n",
    "ta = tr_metrics[\"Average\"]\n",
    "va = va_metrics[\"Average\"]\n",
    "print(\n",
    "    f\"{'Average':<22} \"\n",
    "    f\"{ta['MAE']:>10.4f} {va['MAE']:>10.4f} \"\n",
    "    f\"{ta['RAE']:>10.4f} {va['RAE']:>10.4f} \"\n",
    "    f\"{ta['R2']:>10.4f} {va['R2']:>10.4f} \"\n",
    "    f\"{ta['Spearman R']:>10.4f} {va['Spearman R']:>10.4f} \"\n",
    "    f\"{ta[KT_KEY]:>10.4f} {va[KT_KEY]:>10.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# ---- (0,0) Loss curves (LOG SCALE) ----\n",
    "ax = axes[0, 0]\n",
    "ax.semilogy(history[\"epoch\"], history[\"train_loss\"], label=\"Train Loss\", alpha=0.8)\n",
    "# Compute val loss from val_mae as proxy (we don't track val loss separately)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss (log scale)\")\n",
    "ax.set_title(\"Training Loss\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# ---- (0,1) RAE (capped at 1.0) ----\n",
    "ax = axes[0, 1]\n",
    "ax.plot(history[\"epoch\"], history[\"val_rae\"], \"b-\", alpha=0.8, label=\"Val RAE\")\n",
    "if history[\"logged_epoch\"]:\n",
    "    ax.plot(history[\"logged_epoch\"], history[\"train_rae\"], \"b--\", alpha=0.5, label=\"Train RAE\")\n",
    "best_rae = min(history[\"val_rae\"])\n",
    "ax.axhline(best_rae, color=\"r\", linestyle=\"--\", alpha=0.5, label=f\"Best Val: {best_rae:.3f}\")\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"RAE\")\n",
    "ax.set_title(\"RAE (capped at 1.0)\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# ---- (1,0) R² (capped at 1.0) ----\n",
    "ax = axes[1, 0]\n",
    "ax.plot(history[\"epoch\"], history[\"val_r2\"], \"g-\", alpha=0.8, label=\"Val R²\")\n",
    "if history[\"logged_epoch\"]:\n",
    "    ax.plot(history[\"logged_epoch\"], history[\"train_r2\"], \"g--\", alpha=0.5, label=\"Train R²\")\n",
    "best_r2 = max(history[\"val_r2\"])\n",
    "ax.axhline(best_r2, color=\"r\", linestyle=\"--\", alpha=0.5, label=f\"Best Val: {best_r2:.3f}\")\n",
    "ax.set_ylim(-0.1, 1.0)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"R²\")\n",
    "ax.set_title(\"R² (capped at 1.0)\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# ---- (1,1) Kendall τ ----\n",
    "ax = axes[1, 1]\n",
    "ax.plot(history[\"epoch\"], history[\"val_kendall\"], \"m-\", alpha=0.8, label=\"Val τ\")\n",
    "if history[\"logged_epoch\"]:\n",
    "    ax.plot(history[\"logged_epoch\"], history[\"train_kendall\"], \"m--\", alpha=0.5, label=\"Train τ\")\n",
    "best_tau = max(history[\"val_kendall\"])\n",
    "ax.axhline(best_tau, color=\"r\", linestyle=\"--\", alpha=0.5, label=f\"Best Val: {best_tau:.3f}\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Kendall τ\")\n",
    "ax.set_title(\"Kendall τ\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Bar Plots — Train vs Validation MAE and R²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build metrics dataframe for bar plots\n",
    "# Use the endpoint keys from the metrics dicts directly, so this cell works\n",
    "# regardless of whether the global ENDPOINTS has been overwritten.\n",
    "_train_eps = [k for k in tr_metrics if k != \"Average\"]\n",
    "\n",
    "metric_records = []\n",
    "for ep in _train_eps:\n",
    "    tm = tr_metrics.get(ep, {})\n",
    "    vm = va_metrics.get(ep, {})\n",
    "    metric_records.append({\"Assay\": ep, \"Split\": \"Train\", \"R2\": tm.get(\"R2\", np.nan), \"MAE\": tm.get(\"MAE\", np.nan)})\n",
    "    metric_records.append({\"Assay\": ep, \"Split\": \"Val\", \"R2\": vm.get(\"R2\", np.nan), \"MAE\": vm.get(\"MAE\", np.nan)})\n",
    "\n",
    "metrics_df = pd.DataFrame(metric_records)\n",
    "\n",
    "# Order assays by validation MAE ascending\n",
    "order_by_val = (\n",
    "    metrics_df[metrics_df[\"Split\"] == \"Val\"]\n",
    "    .sort_values(\"MAE\")[\"Assay\"]\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# ---- MAE bar plot ----\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "sns.barplot(x=\"Assay\", y=\"MAE\", hue=\"Split\", data=metrics_df, ax=ax, order=order_by_val)\n",
    "ax.set_ylabel(\"MAE (log-space)\")\n",
    "ax.set_title(\"Training vs Validation MAE per Endpoint\")\n",
    "ax.set_xticklabels([t.get_text().replace(\"_\", \"\\n\") for t in ax.get_xticklabels()])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- R² bar plot ----\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "sns.barplot(x=\"Assay\", y=\"R2\", hue=\"Split\", data=metrics_df, ax=ax, order=order_by_val)\n",
    "ax.set_ylim(-1.1, 1.0)\n",
    "ax.set_ylabel(\"$R^2$\")\n",
    "ax.set_title(\"Training vs Validation R² per Endpoint\")\n",
    "ax.set_xticklabels([t.get_text().replace(\"_\", \"\\n\") for t in ax.get_xticklabels()])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test Set Prediction & Submission\n",
    "\n",
    "Generate predictions on the held-out test set and save as `submission.csv`.\n",
    "The model predicts in **log-space** (matching the training labels), and we\n",
    "inverse-transform to **original assay units** for the submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load test set ---\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "print(f\"Columns: {test_df.columns.tolist()}\")\n",
    "\n",
    "smiles_test = test_df[\"SMILES\"].tolist()\n",
    "\n",
    "# --- Build PyG dataset (inference — no labels) ---\n",
    "print(\"\\nFeaturising test SMILES...\")\n",
    "test_dataset = get_tensor_data(smiles_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# --- Run inference ---\n",
    "model.eval()\n",
    "test_preds_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = batch.to(DEVICE)\n",
    "        pred, _ = model(\n",
    "            x=batch.x,\n",
    "            edge_index=batch.edge_index,\n",
    "            edge_attr=batch.edge_attr,\n",
    "            batch=batch.batch,\n",
    "        )\n",
    "        test_preds_list.append(pred.cpu().numpy())\n",
    "\n",
    "test_preds_log = np.concatenate(test_preds_list, axis=0)  # [N, 9] in log-space\n",
    "print(f\"Predictions shape: {test_preds_log.shape}\")\n",
    "\n",
    "# --- Inverse-transform from log-space to original assay units ---\n",
    "# Forward transform was: log10((y + 1) * multiplier)\n",
    "# Inverse:              (10^x / multiplier) - 1\n",
    "# KSOL and Caco-2 Papp A>B have multiplier = 1e-6; all others have multiplier = 1\n",
    "INVERSE_MAP = {\n",
    "    \"LogD\":             (\"LogD\",                          lambda x: x),\n",
    "    \"LogS\":             (\"KSOL\",                          lambda x: (10**x / 1e-6) - 1),\n",
    "    \"Log_HLM_CLint\":    (\"HLM CLint\",                     lambda x: 10**x - 1),\n",
    "    \"Log_MLM_CLint\":    (\"MLM CLint\",                     lambda x: 10**x - 1),\n",
    "    \"Log_Caco_Papp_AB\": (\"Caco-2 Permeability Papp A>B\",  lambda x: (10**x / 1e-6) - 1),\n",
    "    \"Log_Caco_ER\":      (\"Caco-2 Permeability Efflux\",    lambda x: 10**x - 1),\n",
    "    \"Log_Mouse_PPB\":    (\"MPPB\",                          lambda x: 10**x - 1),\n",
    "    \"Log_Mouse_BPB\":    (\"MBPB\",                          lambda x: 10**x - 1),\n",
    "    \"Log_Mouse_MPB\":    (\"MGMB\",                          lambda x: 10**x - 1),\n",
    "}\n",
    "\n",
    "# Build submission DataFrame\n",
    "sub_df = test_df[[\"Molecule Name\", \"SMILES\"]].copy()\n",
    "\n",
    "# Iterate over INVERSE_MAP keys (log-space names) — NOT the global ENDPOINTS,\n",
    "# which may have been overwritten by the evaluation helpers cell.\n",
    "for i, ep_log in enumerate(INVERSE_MAP):\n",
    "    col_name, transform = INVERSE_MAP[ep_log]\n",
    "    sub_df[col_name] = transform(test_preds_log[:, i])\n",
    "\n",
    "sub_df.to_csv(\"submission.csv\", index=False)\n",
    "print(f\"\\nSaved submission.csv with {len(sub_df)} rows\")\n",
    "print(f\"Columns: {sub_df.columns.tolist()}\\n\")\n",
    "\n",
    "# --- Show head and summary ---\n",
    "print(\"Head:\")\n",
    "print(sub_df.head().to_string())\n",
    "\n",
    "print(\"\\nPrediction range summary (original scale):\")\n",
    "for col in sub_df.columns:\n",
    "    if col not in (\"Molecule Name\", \"SMILES\"):\n",
    "        vals = sub_df[col]\n",
    "        print(f\"  {col:<35s}  min={vals.min():.4f}  max={vals.max():.4f}  mean={vals.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Test Set Evaluation — Leaderboard vs Private\n",
    "\n",
    "Evaluate the submission against the full test-set ground truth, split three ways:\n",
    "\n",
    "| Split | Description |\n",
    "|-------|-------------|\n",
    "| **LB** | Public leaderboard molecules (`is_leaderboard == 1`) |\n",
    "| **Private** | Private test molecules (`is_leaderboard == 0`) |\n",
    "| **All** | Full test set |\n",
    "\n",
    "We report **RAE** (Relative Absolute Error) per endpoint and the **macro-averaged\n",
    "RAE** (MA-RAE) across all 9 endpoints, with bootstrap confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Constants & evaluation helpers\n",
    "# Verbatim from: constants.py, evaluate.py\n",
    "# (matches OpenADMET/ExpansionRx-Challenge-Tutorial eval/ pipeline)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "from typing import Tuple, Dict, Optional\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# --- constants.py -----------------------------------------------------------\n",
    "# Renamed to EVAL_ENDPOINTS to avoid overwriting the training ENDPOINTS\n",
    "# (log-space names) defined in Cell 6 — which Cell 23 relies on.\n",
    "\n",
    "EVAL_ENDPOINTS = [\n",
    "    \"LogD\",\n",
    "    \"KSOL\",\n",
    "    \"MLM CLint\",\n",
    "    \"HLM CLint\",\n",
    "    \"Caco-2 Permeability Efflux\",\n",
    "    \"Caco-2 Permeability Papp A>B\",\n",
    "    \"MPPB\",\n",
    "    \"MBPB\",\n",
    "    \"MGMB\"\n",
    "]\n",
    "\n",
    "METRICS = [\"MAE\", \"RAE\", \"R2\", \"Spearman R\", \"Kendall's Tau\"]\n",
    "\n",
    "# --- evaluate.py -------------------------------------------------------------\n",
    "\n",
    "\n",
    "def clip_and_log_transform(y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Clip to a detection limit and transform to log10 scale.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : np.ndarray\n",
    "        The array to be clipped and transformed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The log10(y + 1) transformed array, with values clipped to >= 0.\n",
    "    \"\"\"\n",
    "    y = np.clip(y, a_min=0, a_max=None)\n",
    "    return np.log10(y + 1)\n",
    "\n",
    "\n",
    "def bootstrap_sampling(size: int, n_samples: int, seed: int = 0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate bootstrap sample indices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    size : int\n",
    "        The size of the original data.\n",
    "    n_samples : int\n",
    "        The number of bootstrap samples to generate.\n",
    "    seed : int, optional\n",
    "        Random seed for reproducibility, by default 0\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Array of shape (n_samples, size) with bootstrap indices.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    return rng.choice(size, size=(n_samples, size), replace=True)\n",
    "\n",
    "\n",
    "def metrics_per_ep(pred: np.ndarray, true: np.ndarray) -> Tuple[float, float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Calculate evaluation metrics for a single sample.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pred : np.ndarray\n",
    "        Array with predictions\n",
    "    true : np.ndarray\n",
    "        Array with actual values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[float, float, float, float, float]\n",
    "        Resulting metrics: (MAE, RAE, R2, Spearman R, Kendall's Tau)\n",
    "    \"\"\"\n",
    "    mae = mean_absolute_error(true, pred)\n",
    "    rae = mae / np.mean(np.abs(true - np.mean(true)))\n",
    "\n",
    "    if np.nanstd(true) == 0:\n",
    "        r2 = np.nan\n",
    "    else:\n",
    "        r2 = r2_score(true, pred)\n",
    "\n",
    "    spr = spearmanr(true, pred).statistic\n",
    "    ktau = kendalltau(true, pred).statistic\n",
    "\n",
    "    return mae, rae, r2, spr, ktau\n",
    "\n",
    "\n",
    "def bootstrap_metrics(\n",
    "    pred: np.ndarray,\n",
    "    true: np.ndarray,\n",
    "    endpoint: str,\n",
    "    n_bootstrap_samples: int = 1000\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate bootstrap metrics given predicted and true values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pred : np.ndarray\n",
    "        Predicted endpoint values\n",
    "    true : np.ndarray\n",
    "        Actual endpoint values\n",
    "    endpoint : str\n",
    "        Name of the endpoint\n",
    "    n_bootstrap_samples : int, optional\n",
    "        Number of bootstrap samples, by default 1000\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with columns [Sample, Endpoint, Metric, Value]\n",
    "        containing estimated metric per bootstrap sample for the given endpoint.\n",
    "    \"\"\"\n",
    "    cols = [\"Sample\", \"Endpoint\", \"Metric\", \"Value\"]\n",
    "    results = []\n",
    "\n",
    "    for i, indx in enumerate(bootstrap_sampling(true.shape[0], n_bootstrap_samples)):\n",
    "        mae, rae, r2, spr, ktau = metrics_per_ep(pred[indx], true[indx])\n",
    "        results.extend([\n",
    "            [i, endpoint, \"MAE\", mae],\n",
    "            [i, endpoint, \"RAE\", rae],\n",
    "            [i, endpoint, \"R2\", r2],\n",
    "            [i, endpoint, \"Spearman R\", spr],\n",
    "            [i, endpoint, \"Kendall's Tau\", ktau]\n",
    "        ])\n",
    "\n",
    "    return pd.DataFrame(results, columns=cols)\n",
    "\n",
    "\n",
    "def _check_required_columns(df: pd.DataFrame, name: str, cols: list[str]) -> None:\n",
    "    \"\"\"Check that all required columns are present in the DataFrame.\"\"\"\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{name} is missing required columns: {missing}\")\n",
    "\n",
    "\n",
    "def calculate_metrics(\n",
    "    predictions_df: pd.DataFrame,\n",
    "    ground_truth_df: pd.DataFrame,\n",
    "    n_bootstrap_samples: int = 1000,\n",
    "    endpoints: Optional[list[str]] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate evaluation metrics for all endpoints.\n",
    "\n",
    "    This is the main evaluation function that:\n",
    "    1. Merges predictions with ground truth on 'Molecule Name'\n",
    "    2. For each endpoint, applies log10(y+1) transform (except LogD)\n",
    "    3. Calculates bootstrap metrics\n",
    "    4. Returns summary statistics (mean +/- std) for each endpoint\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    predictions_df : pd.DataFrame\n",
    "        DataFrame with predictions. Must have 'Molecule Name' column and\n",
    "        one column per endpoint.\n",
    "    ground_truth_df : pd.DataFrame\n",
    "        DataFrame with ground truth values. Must have 'Molecule Name' column\n",
    "        and one column per endpoint.\n",
    "    n_bootstrap_samples : int, optional\n",
    "        Number of bootstrap samples for confidence intervals, by default 1000\n",
    "    endpoints : list[str], optional\n",
    "        List of endpoints to evaluate. If None, uses all EVAL_ENDPOINTS.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with columns [Endpoint, mean_MAE, std_MAE, mean_RAE, std_RAE,\n",
    "        mean_R2, std_R2, mean_Spearman R, std_Spearman R, mean_Kendall's Tau,\n",
    "        std_Kendall's Tau]. Includes a 'Macro Average' row at the end.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If required columns are missing or data validation fails.\n",
    "    \"\"\"\n",
    "    if endpoints is None:\n",
    "        endpoints = EVAL_ENDPOINTS\n",
    "\n",
    "    # Check required columns\n",
    "    _check_required_columns(predictions_df, \"Predictions file\", [\"Molecule Name\"] + endpoints)\n",
    "    _check_required_columns(ground_truth_df, \"Ground truth file\", [\"Molecule Name\"] + endpoints)\n",
    "\n",
    "    # Check all molecules in predictions are in ground truth\n",
    "    if not predictions_df['Molecule Name'].isin(ground_truth_df['Molecule Name']).all():\n",
    "        raise ValueError(\"Predictions file contains molecules not in ground truth\")\n",
    "\n",
    "    # Check for duplicates in predictions\n",
    "    if predictions_df['Molecule Name'].duplicated().any():\n",
    "        raise ValueError(\"Predictions file contains duplicated molecules\")\n",
    "\n",
    "    # Merge dataframes\n",
    "    merged_df = predictions_df.merge(\n",
    "        ground_truth_df,\n",
    "        on=\"Molecule Name\",\n",
    "        suffixes=('_pred', '_true'),\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    merged_df = merged_df.sort_values(\"Molecule Name\")\n",
    "\n",
    "    all_endpoint_results = []\n",
    "\n",
    "    for ept in endpoints:\n",
    "        pred_col = f\"{ept}_pred\"\n",
    "        true_col = f\"{ept}_true\"\n",
    "\n",
    "        # Cast to numeric\n",
    "        merged_df[pred_col] = pd.to_numeric(merged_df[pred_col], errors=\"coerce\")\n",
    "        merged_df[true_col] = pd.to_numeric(merged_df[true_col], errors=\"coerce\")\n",
    "\n",
    "        if merged_df[pred_col].isnull().all():\n",
    "            raise ValueError(f\"All predictions are missing for endpoint {ept}\")\n",
    "\n",
    "        # Subset and drop NaNs\n",
    "        subset = merged_df[[pred_col, true_col]].dropna()\n",
    "        if subset.empty:\n",
    "            raise ValueError(f\"No valid data available for endpoint {ept} after removing NaNs\")\n",
    "\n",
    "        # Extract numpy arrays\n",
    "        y_pred = subset[pred_col].to_numpy()\n",
    "        y_true = subset[true_col].to_numpy()\n",
    "\n",
    "        # Apply log10(y+1) transform except for LogD\n",
    "        if ept.lower() not in ['logd']:\n",
    "            y_true_log = clip_and_log_transform(y_true)\n",
    "            y_pred_log = clip_and_log_transform(y_pred)\n",
    "        else:\n",
    "            y_true_log = y_true\n",
    "            y_pred_log = y_pred\n",
    "\n",
    "        # Calculate bootstrap metrics\n",
    "        bootstrap_df = bootstrap_metrics(y_pred_log, y_true_log, ept, n_bootstrap_samples)\n",
    "\n",
    "        # Pivot to get mean and std\n",
    "        df_endpoint = bootstrap_df.pivot_table(\n",
    "            index=[\"Endpoint\"],\n",
    "            columns=\"Metric\",\n",
    "            values=\"Value\",\n",
    "            aggfunc=[\"mean\", \"std\"]\n",
    "        ).reset_index()\n",
    "\n",
    "        # Flatten column names\n",
    "        df_endpoint.columns = [\n",
    "            f'{i}_{j}' if i != '' else j for i, j in df_endpoint.columns\n",
    "        ]\n",
    "        df_endpoint.rename(columns={'Endpoint_': 'Endpoint'}, inplace=True)\n",
    "\n",
    "        all_endpoint_results.append(df_endpoint)\n",
    "\n",
    "    # Combine all results\n",
    "    df_results = pd.concat(all_endpoint_results, ignore_index=True)\n",
    "\n",
    "    # Calculate macro average\n",
    "    mean_cols = [f'mean_{m}' for m in METRICS]\n",
    "    std_cols = [f'std_{m}' for m in METRICS]\n",
    "\n",
    "    macro_means = df_results[mean_cols].mean()\n",
    "    macro_stds = df_results[std_cols].mean()\n",
    "\n",
    "    avg_row = {\"Endpoint\": \"Macro Average\"}\n",
    "    avg_row.update(macro_means.to_dict())\n",
    "    avg_row.update(macro_stds.to_dict())\n",
    "\n",
    "    df_with_average = pd.concat([df_results, pd.DataFrame([avg_row])], ignore_index=True)\n",
    "\n",
    "    # Fix column order\n",
    "    df_with_average = df_with_average[[\"Endpoint\"] + mean_cols + std_cols]\n",
    "\n",
    "    return df_with_average\n",
    "\n",
    "\n",
    "def get_point_predictions_with_truth(\n",
    "    predictions_df: pd.DataFrame,\n",
    "    ground_truth_df: pd.DataFrame,\n",
    "    endpoint: str\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Get aligned prediction and ground truth arrays for an endpoint.\n",
    "\n",
    "    Useful for scatter plots of predicted vs actual values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    predictions_df : pd.DataFrame\n",
    "        DataFrame with predictions.\n",
    "    ground_truth_df : pd.DataFrame\n",
    "        DataFrame with ground truth values.\n",
    "    endpoint : str\n",
    "        Endpoint name.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[np.ndarray, np.ndarray]\n",
    "        Tuple of (predictions, ground_truth) arrays (log-transformed if applicable).\n",
    "    \"\"\"\n",
    "    merged_df = predictions_df.merge(\n",
    "        ground_truth_df,\n",
    "        on=\"Molecule Name\",\n",
    "        suffixes=('_pred', '_true'),\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    pred_col = f\"{endpoint}_pred\"\n",
    "    true_col = f\"{endpoint}_true\"\n",
    "\n",
    "    merged_df[pred_col] = pd.to_numeric(merged_df[pred_col], errors=\"coerce\")\n",
    "    merged_df[true_col] = pd.to_numeric(merged_df[true_col], errors=\"coerce\")\n",
    "\n",
    "    subset = merged_df[[pred_col, true_col]].dropna()\n",
    "\n",
    "    y_pred = subset[pred_col].to_numpy()\n",
    "    y_true = subset[true_col].to_numpy()\n",
    "\n",
    "    # Apply log10(y+1) transform except for LogD\n",
    "    if endpoint.lower() not in ['logd']:\n",
    "        y_true = clip_and_log_transform(y_true)\n",
    "        y_pred = clip_and_log_transform(y_pred)\n",
    "\n",
    "    return y_pred, y_true\n",
    "\n",
    "\n",
    "def format_metrics_table(df: pd.DataFrame, precision: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Format the metrics DataFrame with mean +/- std format.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame from calculate_metrics().\n",
    "    precision : int, optional\n",
    "        Number of decimal places, by default 3\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with formatted columns like \"MAE\" = \"0.123 +/- 0.012\"\n",
    "    \"\"\"\n",
    "    formatted_df = df[['Endpoint']].copy()\n",
    "\n",
    "    for metric in METRICS:\n",
    "        mean_col = f'mean_{metric}'\n",
    "        std_col = f'std_{metric}'\n",
    "        formatted_df[metric] = df.apply(\n",
    "            lambda row: f\"{row[mean_col]:.{precision}f} +/- {row[std_col]:.{precision}f}\",\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    return formatted_df\n",
    "\n",
    "\n",
    "print(\"Evaluation helpers loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Evaluate submission on LB / Private / All splits\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Load ground truth (includes is_leaderboard flag)\n",
    "gt_df = pd.read_csv(TEST_CSV)\n",
    "submission = pd.read_csv(\"submission.csv\")\n",
    "\n",
    "gt_lb      = gt_df[gt_df[\"is_leaderboard\"] == 1].copy()\n",
    "gt_private = gt_df[gt_df[\"is_leaderboard\"] == 0].copy()\n",
    "gt_all     = gt_df.copy()\n",
    "\n",
    "print(f\"Split sizes — LB: {len(gt_lb)}, Private: {len(gt_private)}, All: {len(gt_all)}\")\n",
    "print(\"Running bootstrap evaluation (1000 samples per split)...\\n\")\n",
    "\n",
    "splits = {\n",
    "    \"LB\":      gt_lb,\n",
    "    \"Private\": gt_private,\n",
    "    \"All\":     gt_all,\n",
    "}\n",
    "\n",
    "split_results = {}\n",
    "for split_name, gt_subset in splits.items():\n",
    "    print(f\"  Evaluating {split_name}...\")\n",
    "    sub_subset = submission[submission[\"Molecule Name\"].isin(gt_subset[\"Molecule Name\"])].copy()\n",
    "    split_results[split_name] = calculate_metrics(sub_subset, gt_subset)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Print formatted metrics per split\n",
    "# ------------------------------------------------------------------\n",
    "for split_name in [\"LB\", \"Private\", \"All\"]:\n",
    "    formatted = format_metrics_table(split_results[split_name])\n",
    "    print(f\"\\n{'=' * 100}\")\n",
    "    print(f\"  {split_name} Split  (mean +/- bootstrap std, 1000 samples)\")\n",
    "    print(f\"{'=' * 100}\")\n",
    "    print(formatted.to_string(index=False))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Highlight MA-RAE per split\n",
    "# ------------------------------------------------------------------\n",
    "print(f\"\\n{'—' * 60}\")\n",
    "print(\"Macro-Averaged RAE per Split:\")\n",
    "for split_name in [\"LB\", \"Private\", \"All\"]:\n",
    "    df = split_results[split_name]\n",
    "    ma = df[df[\"Endpoint\"] == \"Macro Average\"].iloc[0]\n",
    "    print(f\"  MA-RAE ({split_name:>7s}): {ma['mean_RAE']:.3f} +/- {ma['std_RAE']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Scatter plots: Predicted vs True for each endpoint (3x3 grid)\n",
    "# Values are in log10(y+1) space (except LogD which is as-is)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "gt_df = pd.read_csv(TEST_CSV)\n",
    "submission = pd.read_csv(\"submission.csv\")\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for i, ept in enumerate(EVAL_ENDPOINTS):\n",
    "    ax = axes_flat[i]\n",
    "    y_pred, y_true = get_point_predictions_with_truth(submission, gt_df, ept)\n",
    "\n",
    "    ax.scatter(y_true, y_pred, alpha=0.3, s=10, color=\"steelblue\")\n",
    "\n",
    "    # Diagonal reference line\n",
    "    lo = min(y_true.min(), y_pred.min())\n",
    "    hi = max(y_true.max(), y_pred.max())\n",
    "    margin = (hi - lo) * 0.05\n",
    "    ax.plot([lo - margin, hi + margin], [lo - margin, hi + margin],\n",
    "            \"r--\", alpha=0.7, lw=1)\n",
    "\n",
    "    # R² annotation\n",
    "    r2 = r2_score(y_true, y_pred) if np.nanstd(y_true) > 0 else np.nan\n",
    "    ax.annotate(\n",
    "        f\"R² = {r2:.3f}\", xy=(0.05, 0.95), xycoords=\"axes fraction\",\n",
    "        fontsize=11, ha=\"left\", va=\"top\",\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"wheat\", alpha=0.5),\n",
    "    )\n",
    "\n",
    "    label = \"log₁₀(y + 1)\" if ept.lower() != \"logd\" else \"value\"\n",
    "    ax.set_xlabel(f\"True ({label})\")\n",
    "    ax.set_ylabel(f\"Predicted ({label})\")\n",
    "    ax.set_title(ept, fontsize=12)\n",
    "\n",
    "plt.suptitle(\"Predicted vs True — Test Set (All)\", fontsize=14, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gt-pyg)",
   "language": "python",
   "name": "gt-pyg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

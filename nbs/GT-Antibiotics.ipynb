{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data from the paper: [Discovery of a structural class of antibiotics with explainable deep learning](https://www.nature.com/articles/s41586-023-06887-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pawel/anaconda3/envs/gt/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version: 1.21.6\n",
      "Rdkit version: 2022.09.5\n",
      "Torch version: 1.13.1\n",
      "TorchMetrics version: 0.11.4\n"
     ]
    }
   ],
   "source": [
    "# Standard\n",
    "import logging\n",
    "import importlib\n",
    "\n",
    "# Third party\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import RDLogger\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torchmetrics\n",
    "from torchmetrics import AUROC\n",
    "\n",
    "# GT-PyG\n",
    "import gt_pyg\n",
    "from gt_pyg.data.utils import (\n",
    "    get_tensor_data,\n",
    "    get_node_dim,\n",
    "    get_edge_dim,\n",
    "    clean_df,\n",
    ")\n",
    "from gt_pyg.nn.model import GraphTransformerNet\n",
    "\n",
    "\n",
    "# Turn off majority of RDKit warnings\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)\n",
    "\n",
    "\n",
    "# Set a random seed for a reproducibility purposes\n",
    "torch.manual_seed(192837465)\n",
    "\n",
    "# Setup the logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Log the used versions of RDkit and torch\n",
    "print(f'Numpy version: {np.__version__}')\n",
    "print(f'Rdkit version: {rdkit.__version__}')\n",
    "print(f'Torch version: {torch.__version__}')\n",
    "print(f'TorchMetrics version: {torchmetrics.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The training dataset\n",
    "\n",
    "**Note**: 39,312 compounds for the antibacterial activity againts _S. aureus_ (Gram-positive pathogen)\n",
    "\n",
    "The training set was downloaded from the [original repo](https://github.com/felixjwong/antibioticsai) on 12/31/2023.\n",
    "\n",
    "Reference: _Wong et al., Nature, 2023_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nc1nnc(o1)-c1ccc(o1)[N+](=O)[O-]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O[C@H]1COC[C@@H]2O[C@H](CC[C@H]2N(C1)C(=O)Nc1c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CC(C)C[C@@H](N)C(=O)N[C@@H]1[C@H](O)c2ccc(c(c2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[O-][N+](=O)c1ccc(o1)/C=N/N1CC(=O)NC1=O</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cn1cnc(c1)CCNC(=O)C[C@@H]1CC[C@@H]2[C@H](COC[C...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              smiles  activity\n",
       "0                   Nc1nnc(o1)-c1ccc(o1)[N+](=O)[O-]         1\n",
       "1  O[C@H]1COC[C@@H]2O[C@H](CC[C@H]2N(C1)C(=O)Nc1c...         1\n",
       "2  CC(C)C[C@@H](N)C(=O)N[C@@H]1[C@H](O)c2ccc(c(c2...         1\n",
       "3            [O-][N+](=O)c1ccc(o1)/C=N/N1CC(=O)NC1=O         1\n",
       "4  Cn1cnc(c1)CCNC(=O)C[C@@H]1CC[C@@H]2[C@H](COC[C...         1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_filename = './data/train.antibioticsai.csv'\n",
    "df = pd.read_csv(dataset_filename, sep=',')\n",
    "df = df.rename(columns={'SMILES': 'smiles', 'ACTIVITY': 'activity'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper seems to be employing either a random or stratified split for evaluating the model. In the exercise below, we opt for the random split. We generate training, validation, and test sets, maintaining the same proportions as outlined in the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_split(df, frac=[0.8, 0.1, 0.1], min_num_atoms=0):\n",
    "    \n",
    "    # Assert that the sum of fractions is equal to 1\n",
    "    assert sum(frac) == 1, \"The sum of fractions must be equal to 1\"\n",
    "\n",
    "    # Shuffle the DataFrame rows to get a random split\n",
    "    shuffled_df = df.sample(frac=1, random_state=42)\n",
    "\n",
    "    # Calculate the number of rows for each set\n",
    "    total_rows = len(shuffled_df)\n",
    "    train_rows = int(frac[0] * total_rows)\n",
    "    val_rows = int(frac[1] * total_rows)\n",
    "\n",
    "    # Split the DataFrame\n",
    "    train_set = shuffled_df.iloc[:train_rows]\n",
    "    val_set = shuffled_df.iloc[train_rows:train_rows + val_rows]\n",
    "    test_set = shuffled_df.iloc[train_rows + val_rows:]\n",
    "\n",
    "    train_set = clean_df(train_set, min_num_atoms=min_num_atoms, x_label='smiles', y_label='activity')\n",
    "    val_set = clean_df(val_set, min_num_atoms=min_num_atoms, x_label='smiles', y_label='activity')\n",
    "    test_set = clean_df(test_set, min_num_atoms=min_num_atoms, x_label='smiles', y_label='activity')\n",
    "    \n",
    "    return (train_set, val_set, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the minimum number of atoms in a molecule to 4, as we utilize 6 eigenvectors for spectral positional encoding. It is important to note that this exclusion affects compounds in both the training and validation sets, but not in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Removed 7 compounds that did not meet the size criteria.\n",
      "INFO:root:Removed 1 compounds that did not meet the size criteria.\n",
      "INFO:root:Removed 0 compounds that did not meet the size criteria.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 31442\n",
      "Number of validation examples: 3930\n",
      "Number of test examples: 3932\n"
     ]
    }
   ],
   "source": [
    "PE_DIM = 6\n",
    "(tr, va, te) = get_random_split(df, frac=[0.8, 0.1, 0.1], min_num_atoms=4)\n",
    "tr_dataset = get_tensor_data(tr.smiles.to_list(), tr.activity.to_list(), pe_dim=PE_DIM)\n",
    "va_dataset = get_tensor_data(va.smiles.to_list(), va.activity.to_list(), pe_dim=PE_DIM)\n",
    "te_dataset = get_tensor_data(te.smiles.to_list(), te.activity.to_list(), pe_dim=PE_DIM)\n",
    "NODE_DIM = get_node_dim()\n",
    "EDGE_DIM = get_edge_dim()\n",
    "\n",
    "print(f'Number of training examples: {len(tr_dataset)}')\n",
    "print(f'Number of validation examples: {len(va_dataset)}')\n",
    "print(f'Number of test examples: {len(te_dataset)}')\n",
    "\n",
    "train_loader = DataLoader(tr_dataset, batch_size=256)\n",
    "val_loader = DataLoader(va_dataset, batch_size=1024)\n",
    "test_loader = DataLoader(te_dataset, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 8 compounds.\n"
     ]
    }
   ],
   "source": [
    "original_number_of_cmpds = len(df)\n",
    "num_tr = len(tr)\n",
    "num_ve = len(va)\n",
    "num_te = len(te)\n",
    "print(f'Removed {original_number_of_cmpds - num_tr - num_ve - num_te} compounds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and eval the GT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions\n",
    "\n",
    "Due to the significant imbalance in the training set, we apply an upweighting technique to the positive class during the calculation of the loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "def calculate_class_ratio(labels):\n",
    "    pos_class = sum(labels)\n",
    "    neg_class = len(labels) - pos_class\n",
    "    \n",
    "    ratio = max(1, neg_class) / max(1, pos_class)\n",
    "    return torch.tensor(ratio)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    auroc = AUROC(task=\"binary\")\n",
    "\n",
    "    for data in train_loader:\n",
    "        pos_weight = calculate_class_ratio(data.y)\n",
    "        loss_func = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        \n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # randomly flip sign of eigenvectors\n",
    "        batch_pe = data.pe * (2 * torch.randint(low=0, high=2, size=(1, PE_DIM)).float() - 1.0)\n",
    "        (out, _) = model(data.x, data.edge_index, data.edge_attr, batch_pe, data.batch, zero_var=True)\n",
    "        loss = loss_func(out.squeeze(), data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        auroc.update(out.squeeze(), data.y)\n",
    "\n",
    "    return auroc.compute()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    auroc = AUROC(task=\"binary\")\n",
    "    \n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        # randomly flip sign of eigenvectors\n",
    "        batch_pe = data.pe * (2 * torch.randint(low=0, high=2, size=(1, PE_DIM)).float() - 1.0)\n",
    "        (out,_) = model(data.x, data.edge_index, data.edge_attr, batch_pe, data.batch)\n",
    "        \n",
    "        auroc.update(out.squeeze(), data.y)\n",
    "        \n",
    "    return auroc.compute()\n",
    "\n",
    "@torch.no_grad()\n",
    "def calc_auprc(loader):\n",
    "    (scores, labels) = score(loader)\n",
    "    precision, recall, thresholds = precision_recall_curve(labels, scores)\n",
    "    return (auc(recall, precision), scores, labels)\n",
    "\n",
    "@torch.no_grad()\n",
    "def score(loader):\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            batch_pe = data.pe * (2 * torch.randint(low=0, high=2, size=(1, PE_DIM)).float() - 1.0)\n",
    "            logits, _ = model(data.x, data.edge_index, data.edge_attr, batch_pe, data.batch)\n",
    "            all_logits.append(logits.cpu().numpy())\n",
    "            all_labels.append(data.y.cpu().numpy())\n",
    "\n",
    "    all_logits = np.concatenate(all_logits)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    return (all_logits, all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphTransformer architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphTransformerNet(\n",
      "  (node_emb): Linear(in_features=76, out_features=128, bias=False)\n",
      "  (edge_emb): Linear(in_features=10, out_features=128, bias=False)\n",
      "  (pe_emb): Linear(in_features=6, out_features=128, bias=False)\n",
      "  (gt_layers): ModuleList(\n",
      "    (0): GTConv(128, 128, heads=8, aggrs: sum,mean, qkv_bias: False, gate: True)\n",
      "    (1): GTConv(128, 128, heads=8, aggrs: sum,mean, qkv_bias: False, gate: True)\n",
      "    (2): GTConv(128, 128, heads=8, aggrs: sum,mean, qkv_bias: False, gate: True)\n",
      "    (3): GTConv(128, 128, heads=8, aggrs: sum,mean, qkv_bias: False, gate: True)\n",
      "    (4): GTConv(128, 128, heads=8, aggrs: sum,mean, qkv_bias: False, gate: True)\n",
      "  )\n",
      "  (global_pool): MultiAggregation([\n",
      "    SumAggregation(),\n",
      "    MeanAggregation(),\n",
      "    MaxAggregation(),\n",
      "    StdAggregation(),\n",
      "  ], mode=cat)\n",
      "  (mu_mlp): MLP(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (log_var_mlp): MLP(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Number of params: 1219 k\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GraphTransformerNet(node_dim_in=NODE_DIM,\n",
    "                            edge_dim_in=EDGE_DIM,\n",
    "                            pe_in_dim=PE_DIM,\n",
    "                            num_gt_layers=5, \n",
    "                            hidden_dim=128,\n",
    "                            num_heads=8,\n",
    "                            norm='bn',\n",
    "                            gt_aggregators=['sum', \"mean\"],\n",
    "                            aggregators=['sum','mean','max', 'std'],\n",
    "                            dropout=0.1,\n",
    "                            act='relu',\n",
    "                            gate=True).to(device)\n",
    "\n",
    "if int(torch.__version__.split('.')[0]) >= 2:\n",
    "    model = torch_geometric.compile(model) \n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5,\n",
    "                              min_lr=0.00001)\n",
    "\n",
    "print(model)\n",
    "print(f\"Number of params: {model.num_parameters()//1000} k\")\n",
    "\n",
    "\n",
    "best_epoch = 0\n",
    "best_validation_loss = -np.inf\n",
    "test_set_auroc = -np.inf\n",
    "\n",
    "best_labels, best_scores = None, None\n",
    "for epoch in range(1, 3):\n",
    "    tr_auroc = train(epoch)\n",
    "    #va_auroc = test(val_loader)\n",
    "    #te_auroc = test(test_loader)\n",
    "    \n",
    "    (tr_auprc, _, _) = calc_auprc(train_loader)\n",
    "    (va_auprc, _, _) = calc_auprc(val_loader)\n",
    "    (te_auprc, scores, labels) = calc_auprc(test_loader)\n",
    "    scheduler.step(va_auprc)\n",
    "    #print(f'Epoch: {epoch:02d}, AUROC: Train: {tr_auroc:.4f}, Val: {va_auroc:.4f}, '\n",
    "    #      f'Test: {te_auroc:.4f}')\n",
    "    print(f'Epoch: {epoch:02d}, AUPRC= Train: {tr_auprc:.4f} (AUROC= {tr_auroc:.4f}), Val: {va_auprc:.4f}, '\n",
    "          f'Test: {te_auprc:.4f}')\n",
    "        \n",
    "    va_loss = tr_auprc\n",
    "    te_loss = te_auprc\n",
    "    \n",
    "    if va_loss > best_validation_loss:\n",
    "        best_epoch = epoch\n",
    "        best_validation_loss = va_loss\n",
    "        test_set_auroc = te_loss\n",
    "        \n",
    "        best_labels = scores\n",
    "        best_scores = labels\n",
    "        \n",
    "print(\"\\nModel's performance on the test set\\n\"\n",
    "        \"===================================\\n\"\n",
    "        f'AUROC={test_set_auroc}\\n'\n",
    "        f'Epoch={best_epoch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area under Precision-Recall curve (AUPRC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original paper, the AUPRC is approximately ~0.35-0.37; however, an exact comparison is not feasible due to the lack of specific information regarding the training, validation, and test sets. Despite this, in our first attempt, we already achieved performance comparable to the optimized ensemble model used in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate precision and recall values at different probability thresholds\n",
    "precision, recall, thresholds = precision_recall_curve(best_labels, best_scores)\n",
    "\n",
    "# Calculate the area under the precision-recall curve\n",
    "auc_pr = auc(recall, precision)\n",
    "\n",
    "# Add precision-recall curve for a random model\n",
    "random_predictions = np.random.rand(len(best_labels))\n",
    "random_precision, random_recall, _ = precision_recall_curve(best_labels, random_predictions)\n",
    "random_auc_pr = auc(random_recall, random_precision)\n",
    "\n",
    "positive_ratio = np.sum(best_labels) / len(best_labels)\n",
    "random_skill_recall = np.linspace(0, 1, len(best_labels))\n",
    "random_skill_precision = np.full_like(random_skill_recall, positive_ratio)\n",
    "random_skill_auc_pr = auc(random_skill_recall, random_skill_precision)\n",
    "\n",
    "\n",
    "# Plot the precision-recall curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, label=f'AUC-PR = {auc_pr:.2f}')\n",
    "plt.plot(random_skill_recall, random_skill_precision, linestyle='--', label=f'no-skill (AUC-PR = {random_skill_auc_pr:.2f}')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
